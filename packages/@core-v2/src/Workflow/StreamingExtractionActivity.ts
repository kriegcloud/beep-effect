/**
 * Workflow: Streaming Extraction Activity
 *
 * Durable activity wrapper for the 6-phase unified streaming extraction pipeline.
 * This is the single source of truth for all document extraction in batch workflows.
 *
 * Pipeline (delegated to StreamingExtraction):
 * 1. Chunking - Split document into processable segments
 * 2. Mention Detection - Extract entity mention spans
 * 3. Entity Extraction - LLM-based entity typing
 * 4. Property Scoping - Domain/range filtered properties
 * 5. Relation Extraction - LLM-based relation extraction
 * 6. Grounding - Filter relations by embedding similarity (≥0.8)
 *
 * @since 2.0.0
 * @module Workflow/StreamingExtractionActivity
 */

import { Activity } from "@effect/workflow"
import { DateTime, Effect, Option, Schedule } from "effect"
import * as Crypto from "node:crypto"
import { ActivityError, notFoundError, toActivityError } from "../Domain/Error/Activity.js"
import { type BatchId, type ContentHash, type Namespace, type OntologyName, toGcsUri } from "../Domain/Identity.js"
import { Entity, KnowledgeGraph } from "../Domain/Model/Entity.js"
import { ChunkingConfig, LlmConfig, RunConfig } from "../Domain/Model/ExtractionRun.js"
import { OntologyRef } from "../Domain/Model/Ontology.js"
import { PathLayout } from "../Domain/PathLayout.js"
import type { ExtractionActivityInput } from "../Domain/Schema/Batch.js"
// Note: ClaimPersistenceService removed - claims persist only after validation
// via makeClaimPersistenceActivity in WorkflowOrchestrator
import { ConfigService } from "../Service/Config.js"
import { ExtractionWorkflow } from "../Service/ExtractionWorkflow.js"
import { RdfBuilder } from "../Service/Rdf.js"
import { StorageService } from "../Service/Storage.js"
import { claimsDataToQuads, knowledgeGraphToClaims } from "../Utils/ClaimFactory.js"
import { makeProvenanceUri } from "../Utils/Provenance.js"
import { ExtractionActivityOutput } from "./Activities.js"

// -----------------------------------------------------------------------------
// Output Schema
// -----------------------------------------------------------------------------

/**
 * Output schema for StreamingExtractionActivity
 *
 * Re-exports ExtractionActivityOutput from Activities.ts for API compatibility.
 * Both legacy Activities and StreamingExtractionActivity use the same schema.
 */
export const StreamingExtractionOutput = ExtractionActivityOutput
export type StreamingExtractionOutput = typeof ExtractionActivityOutput.Type

// -----------------------------------------------------------------------------
// Helpers
// -----------------------------------------------------------------------------

const stripGsPrefix = (uri: string): string => uri.startsWith("gs://") ? uri.replace(/^gs:\/\/[^/]+\//, "") : uri

const requireContent = (opt: Option.Option<string>, key: string) =>
  Option.match(opt, {
    onNone: () => Effect.fail(notFoundError("StorageObject", key)),
    onSome: (value) => Effect.succeed(value)
  })

const resolveBucket = (config: { storage: { bucket: Option.Option<string> } }) =>
  Option.getOrElse(config.storage.bucket, () => "local-bucket")

/**
 * Default retry policy for activities
 * - Exponential backoff starting at 1 second
 * - Max 3 attempts
 * - Jitter to prevent thundering herd
 */
const activityRetryPolicy = Schedule.exponential("1 second").pipe(
  Schedule.jittered,
  Schedule.compose(Schedule.recurs(3))
)

// -----------------------------------------------------------------------------
// Config Builders
// -----------------------------------------------------------------------------

/**
 * Compute a content hash from a string (16 hex chars)
 *
 * @param content - String to hash
 * @returns 16-character hex hash suitable for ContentHash branded type
 */
const computeContentHash = (content: string): ContentHash =>
  Crypto.createHash("sha256").update(content).digest("hex").slice(0, 16) as ContentHash

/**
 * Extract ontology name from URI path
 *
 * @example
 * extractOntologyName("gs://bucket/ontologies/football/ontology.ttl") => "ontology"
 * extractOntologyName("gs://bucket/seattle.ttl") => "seattle"
 */
const extractOntologyName = (uri: string): OntologyName => {
  const path = stripGsPrefix(uri)
  const filename = path.split("/").pop() ?? "ontology"
  const name = filename.replace(/\.(ttl|rdf|owl|n3)$/, "")
  // Ensure valid OntologyName pattern (alphanumeric + hyphens + underscores)
  const sanitized = name.replace(/[^a-zA-Z0-9_-]/g, "-").toLowerCase()
  return (sanitized || "ontology") as OntologyName
}

/**
 * Build RunConfig from ExtractionActivityInput
 *
 * Translates the batch activity input (with preprocessing hints) to the
 * RunConfig format expected by StreamingExtraction.
 *
 * @param input - Extraction activity input with optional preprocessing hints
 * @param llmConfig - LLM configuration from ConfigService
 * @param ontologyContentHash - Pre-computed hash of ontology CONTENT (not URI)
 * @returns RunConfig for StreamingExtraction
 */
export const buildRunConfig = (
  input: typeof ExtractionActivityInput.Type,
  llmConfig: {
    model: string
    temperature: number
    maxTokens: number
    timeoutMs: number
  },
  ontologyContentHash: ContentHash
): RunConfig => {
  // Build OntologyRef from the ontology URI
  // Use content hash for cache invalidation when ontology changes
  const ontologyRef = new OntologyRef({
    namespace: input.targetNamespace as Namespace,
    name: extractOntologyName(input.ontologyUri),
    contentHash: ontologyContentHash
  })

  // Build ChunkingConfig - use preprocessing hints if available, otherwise defaults
  const chunkingConfig = new ChunkingConfig({
    maxChunkSize: 500, // Default chunk size (TODO: get from preprocessing hints)
    preserveSentences: true,
    overlapTokens: 50
  })

  // Build LlmConfig from service config
  const llmConfigSchema = new LlmConfig({
    model: llmConfig.model,
    temperature: llmConfig.temperature,
    maxTokens: llmConfig.maxTokens,
    timeoutMs: llmConfig.timeoutMs
  })

  return new RunConfig({
    ontology: ontologyRef,
    chunking: chunkingConfig,
    llm: llmConfigSchema,
    concurrency: 5, // Default concurrency
    enableGrounding: true // Always enable grounding for quality
  })
}

/**
 * Enrich extracted entities with document-level metadata
 *
 * Adds provenance information to each entity for traceability.
 *
 * @param entities - Extracted entities from StreamingExtraction
 * @param input - Original extraction input with document metadata
 * @param extractedAt - Timestamp of extraction
 * @returns Enriched entities with document metadata
 */
export const enrichEntityMetadata = (
  entities: ReadonlyArray<Entity>,
  input: typeof ExtractionActivityInput.Type,
  extractedAt: DateTime.Utc
): ReadonlyArray<Entity> =>
  entities.map((entity) =>
    new Entity({
      ...entity,
      documentId: input.documentId,
      sourceUri: input.sourceUri,
      extractedAt,
      // Inherit eventTime from document metadata (if available)
      eventTime: input.eventTime ?? entity.eventTime
    })
  )

// -----------------------------------------------------------------------------
// Streaming Extraction Activity
// -----------------------------------------------------------------------------

/**
 * Durable Streaming Extraction Activity
 *
 * Unified extraction activity that uses the 6-phase streaming extraction pipeline.
 * This replaces makeExtractionActivity in DurableActivities.ts as the canonical
 * extraction path.
 *
 * Key differences from legacy makeExtractionActivity:
 * - Uses StreamingExtraction for the 6-phase pipeline
 * - Grounding verification is always enabled (≥0.8 threshold)
 * - Consistent with streaming/batch unification goal
 *
 * Pipeline:
 * 1. Read source document from storage
 * 2. Build RunConfig from input (with preprocessing hints)
 * 3. Call StreamingExtraction.extract() for 6-phase extraction
 * 4. Enrich entities with document metadata
 * 5. Convert to claims using knowledgeGraphToClaims()
 * 6. Serialize to RDF using claimsDataToQuads()
 * 7. Write graph to storage and return output
 *
 * @param input - Extraction activity input (from batch workflow)
 * @returns Durable activity with journaled execution
 *
 * @since 2.0.0
 */
export const makeStreamingExtractionActivity = (input: typeof ExtractionActivityInput.Type) =>
  Activity.make({
    name: `streaming-extraction-${input.documentId}`,
    success: StreamingExtractionOutput,
    error: ActivityError,
    execute: Effect.gen(function*() {
      const start = yield* DateTime.now
      const storage = yield* StorageService
      const config = yield* ConfigService
      const extractionWorkflow = yield* ExtractionWorkflow
      const rdf = yield* RdfBuilder

      const bucket = resolveBucket(config)

      yield* Effect.logInfo("Streaming extraction activity starting", {
        batchId: input.batchId,
        documentId: input.documentId,
        sourceUri: input.sourceUri,
        ontologyUri: input.ontologyUri
      })

      // 1. Read source document
      const sourceKey = stripGsPrefix(input.sourceUri)
      const sourceContent = yield* storage.get(sourceKey).pipe(
        Effect.flatMap((opt) => requireContent(opt, sourceKey))
      )

      yield* Effect.logInfo("Source document loaded", {
        documentId: input.documentId,
        contentLength: sourceContent.length
      })

      // 2. Load ontology and compute content hash for cache invalidation
      const ontologyKey = stripGsPrefix(input.ontologyUri)
      const ontologyContent = yield* storage.get(ontologyKey).pipe(
        Effect.flatMap((opt) => requireContent(opt, ontologyKey)),
        Effect.mapError((error) => toActivityError(new Error(`Failed to load ontology: ${ontologyKey} - ${error}`)))
      )
      const ontologyContentHash = computeContentHash(ontologyContent)

      yield* Effect.logDebug("Ontology loaded for content hashing", {
        documentId: input.documentId,
        ontologyKey,
        contentHash: ontologyContentHash,
        contentLength: ontologyContent.length
      })

      // 3. Build RunConfig from input with content-based hash
      const runConfig = buildRunConfig(input, {
        model: config.llm.model,
        temperature: 0.0, // Deterministic extraction
        maxTokens: config.llm.maxTokens,
        timeoutMs: config.llm.timeoutMs
      }, ontologyContentHash)

      yield* Effect.logInfo("RunConfig built for streaming extraction", {
        documentId: input.documentId,
        ontologyRef: runConfig.ontology.shortId,
        chunkSize: runConfig.chunking.maxChunkSize,
        concurrency: runConfig.concurrency,
        enableGrounding: runConfig.enableGrounding
      })

      // 4. Run 6-phase streaming extraction
      const rawGraph = yield* extractionWorkflow.extract(sourceContent, runConfig).pipe(
        Effect.withLogSpan("streaming-extraction-6-phase"),
        Effect.tap((graph) =>
          Effect.logInfo("Streaming extraction complete", {
            documentId: input.documentId,
            entityCount: graph.entities.length,
            relationCount: graph.relations.length
          })
        ),
        Effect.mapError((error) =>
          toActivityError(
            error instanceof Error
              ? error
              : new Error(`Streaming extraction failed: ${String(error)}`)
          )
        )
      )

      // 5. Enrich entities with document metadata
      const extractedAt = yield* DateTime.now
      const enrichedEntities = enrichEntityMetadata(
        rawGraph.entities,
        input,
        extractedAt
      )

      const graph = new KnowledgeGraph({
        entities: Array.from(enrichedEntities),
        relations: rawGraph.relations,
        sourceText: sourceContent
      })

      yield* Effect.logInfo("Entities enriched with document metadata", {
        documentId: input.documentId,
        entityCount: graph.entities.length,
        hasEventTime: input.eventTime !== undefined
      })

      // 6. Generate provenance URI and create claims
      const provenanceUri = makeProvenanceUri(
        input.batchId as BatchId,
        input.documentId
      )

      // Serialize with named graph for provenance tracking
      const store = yield* rdf.createStore
      yield* rdf.addEntities(store, graph.entities, {
        graphUri: provenanceUri,
        targetNamespace: input.targetNamespace
      })
      yield* rdf.addRelations(store, graph.relations, {
        graphUri: provenanceUri,
        targetNamespace: input.targetNamespace
      })

      // 7. Create claims from extracted entities and relations
      // Convert Namespace identifier to full IRI
      const match = config.rdf.baseNamespace.match(/^https?:\/\/[^/]+\//)
      const baseDomain = match ? match[0] : "http://example.org/"
      const baseNamespace = `${baseDomain}${input.targetNamespace}/`
      const claims = knowledgeGraphToClaims(
        graph.entities,
        graph.relations,
        {
          baseNamespace,
          documentId: input.documentId,
          ontologyId: input.ontologyId,
          defaultConfidence: 0.85
        }
      )

      // Convert claims to RDF quads and add to store
      const claimQuads = claimsDataToQuads(
        claims,
        provenanceUri,
        extractedAt.toString()
      )

      // Add claim quads to the store
      for (const quad of claimQuads) {
        const n3 = yield* Effect.promise(() => import("n3"))
        const { DataFactory } = n3
        const subject = DataFactory.namedNode(quad.subject as string)
        const predicate = DataFactory.namedNode(quad.predicate as string)

        // Handle object (IRI or Literal)
        let object: ReturnType<typeof DataFactory.namedNode> | ReturnType<typeof DataFactory.literal>
        if (typeof quad.object === "string") {
          object = DataFactory.namedNode(quad.object)
        } else {
          const lit = quad.object as { value: string; datatype?: string; language?: string }
          if (lit.datatype) {
            object = DataFactory.literal(lit.value, DataFactory.namedNode(lit.datatype))
          } else if (lit.language) {
            object = DataFactory.literal(lit.value, lit.language)
          } else {
            object = DataFactory.literal(lit.value)
          }
        }

        const graphNode = quad.graph
          ? DataFactory.namedNode(quad.graph)
          : DataFactory.defaultGraph()

        store._store.addQuad(DataFactory.quad(subject, predicate, object, graphNode))
      }

      yield* Effect.logInfo("Claims created from extraction", {
        documentId: input.documentId,
        claimCount: claims.length,
        entityClaims: graph.entities.length * 2, // type + label claims per entity
        relationClaims: graph.relations.length
      })

      const trigContent = yield* rdf.toTriG(store)

      yield* Effect.logInfo("Graph serialized to TriG with provenance and claims", {
        documentId: input.documentId,
        provenanceUri,
        trigLength: trigContent.length,
        claimCount: claims.length
      })

      // 7. Save TriG graph to storage
      const graphPath = PathLayout.document.graph(input.documentId)
      yield* storage.set(graphPath, trigContent)

      const graphUri = toGcsUri(bucket, graphPath)

      // Note: Claims are persisted only after SHACL validation passes,
      // via makeClaimPersistenceActivity in WorkflowOrchestrator.
      // This ensures only validated claims enter the database.

      const end = yield* DateTime.now

      yield* Effect.logInfo("Streaming extraction activity complete", {
        batchId: input.batchId,
        documentId: input.documentId,
        entityCount: graph.entities.length,
        relationCount: graph.relations.length,
        claimCount: claims.length,
        durationMs: DateTime.distance(start, end)
      })

      return {
        documentId: input.documentId,
        graphUri,
        entityCount: graph.entities.length,
        relationCount: graph.relations.length,
        claimCount: claims.length,
        durationMs: DateTime.distance(start, end)
      }
    }).pipe(Effect.mapError(toActivityError)),
    interruptRetryPolicy: activityRetryPolicy
  })
