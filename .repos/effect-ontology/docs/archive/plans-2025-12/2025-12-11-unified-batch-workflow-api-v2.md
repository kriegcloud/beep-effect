# Unified Batch Workflow API Design (v2)

**Date:** 2025-12-11
**Status:** Ready for implementation
**Supersedes:** 2025-12-11-unified-batch-workflow-api.md

## Summary of Changes from v1

This v2 addresses all review notes with tighter Effect integration:

| Issue | v1 Approach | v2 Resolution |
|-------|-------------|---------------|
| Schema mismatch | Request omitted `batchId`/`createdAt` | Split into `BatchRequest` (input) vs `BatchManifest` (staged) |
| Manifest staging | Assumed pre-existing | Explicit ingress step writes manifest to GCS |
| Workflow annotations | None | Add `SuspendOnFailure`, `CaptureDefects` |
| Execution IDs | Custom `generateBatchId` | Use `Workflow.idempotencyKey` |
| State emission | Only terminal state | Poll-based SSE with `Stream.repeatEffectOption` |
| SSE framing | Manual string concat | `Sse.encoder` + `HttpServerResponse.stream` |
| Compensation | None | `Workflow.withCompensation` for cleanup |
| Concurrency | Hardcoded `5` | Configurable via `ConfigService` |
| HTTP schemas | Ad-hoc | `HttpServerRequest.schemaBodyJson` / `HttpServerResponse.schemaJson` |

---

## 1. Schema Design

### 1.1 Request Schema (API Input)

New schema for API requests - excludes server-generated fields:

```typescript
// Domain/Schema/BatchRequest.ts

import { Schema } from "effect"
import { DocumentId, GcsUri, Namespace, OntologyVersion } from "../Identity.js"

/** Document input for batch request (sizeBytes optional) */
export const RequestDocument = Schema.Struct({
  documentId: Schema.optional(DocumentId), // Server generates if omitted
  sourceUri: GcsUri,
  contentType: Schema.String,
  sizeBytes: Schema.optional(Schema.Number)
})

/** API request body - server generates batchId and createdAt */
export const BatchRequest = Schema.Struct({
  ontologyUri: GcsUri,
  ontologyVersion: OntologyVersion,
  shaclUri: Schema.optional(GcsUri),
  targetNamespace: Namespace,
  documents: Schema.NonEmptyArray(RequestDocument)
})

export type BatchRequest = typeof BatchRequest.Type
```

### 1.2 Manifest Schema (Staged)

Existing `BatchManifest` remains for persistence - populated by server:

```typescript
// Domain/Schema/Batch.ts (existing, unchanged)
export const BatchManifest = Schema.Struct({
  batchId: BatchId,           // Generated by server
  ontologyUri: GcsUri,
  ontologyVersion: OntologyVersion,
  shaclUri: Schema.optional(GcsUri),
  targetNamespace: Namespace,
  documents: Schema.Array(ManifestDocument),
  createdAt: Schema.DateTimeUtc  // Generated by server
})
```

### 1.3 Ingress Transform

```typescript
// Service/BatchIngress.ts

const createManifest = (request: BatchRequest): Effect<BatchManifest> =>
  Effect.gen(function* () {
    const now = yield* DateTime.now

    // Generate IDs for documents missing them
    const documents = yield* Effect.forEach(request.documents, (doc) =>
      Effect.gen(function* () {
        const id = doc.documentId ?? (yield* generateDocumentId())
        return {
          documentId: id,
          sourceUri: doc.sourceUri,
          contentType: doc.contentType,
          sizeBytes: doc.sizeBytes ?? 0
        }
      })
    )

    // Use deterministic batchId from request hash for idempotency
    const batchId = yield* deriveIdempotentBatchId(request)

    return {
      batchId,
      ontologyUri: request.ontologyUri,
      ontologyVersion: request.ontologyVersion,
      shaclUri: request.shaclUri,
      targetNamespace: request.targetNamespace,
      documents,
      createdAt: now
    }
  })
```

---

## 2. Manifest Staging Contract

Before invoking workflow, the API writes manifest to storage:

```typescript
// Service/BatchIngress.ts

export const stageManifest = (manifest: BatchManifest): Effect<GcsUri, SystemError> =>
  Effect.gen(function* () {
    const storage = yield* StorageService
    const config = yield* ConfigService

    // Path: batches/{batchId}/manifest.json
    const manifestPath = `batches/${manifest.batchId}/manifest.json`
    const manifestJson = JSON.stringify(Schema.encodeSync(BatchManifest)(manifest))

    yield* storage.set(manifestPath, manifestJson)

    // Return full GCS URI
    return `gs://${config.storage.bucket}/${manifestPath}` as GcsUri
  })
```

### 2.1 Full Ingress Flow

```typescript
export const startBatchExtraction = (request: BatchRequest) =>
  Effect.gen(function* () {
    // 1. Create manifest with generated IDs
    const manifest = yield* createManifest(request)

    // 2. Stage to storage
    const manifestUri = yield* stageManifest(manifest)

    // 3. Build workflow payload
    const payload: BatchWorkflowPayload = {
      batchId: manifest.batchId,
      manifestUri,
      ontologyVersion: manifest.ontologyVersion
    }

    // 4. Start workflow (idempotent via batchId)
    const orchestrator = yield* WorkflowOrchestrator
    const executionId = yield* orchestrator.start(payload)

    return { batchId: manifest.batchId, executionId, manifestUri }
  })
```

---

## 3. Workflow Annotations

### 3.1 Updated Workflow Definition

```typescript
// Service/WorkflowOrchestrator.ts

import { Context, Schema } from "effect"
import { Workflow } from "@effect/workflow"

export const BatchExtractionWorkflow = Workflow.make({
  name: "batch-extraction",
  payload: BatchWorkflowPayload,
  success: BatchState,
  error: Schema.String,

  // Use payload hash for idempotency
  idempotencyKey: (p) => p.batchId,

  // Annotations for resumability and defect capture
  annotations: Context.make(Workflow.SuspendOnFailure, true).pipe(
    Context.add(Workflow.CaptureDefects, true)
  )
})
```

### 3.2 SuspendOnFailure Behavior

When workflow fails:
1. State persisted to PostgreSQL as `Workflow.Suspended`
2. Can be resumed via `WorkflowOrchestrator.resume(executionId)`
3. SSE returns `BatchFailed` with `isResumable: true`

### 3.3 CaptureDefects Behavior

Unexpected errors (defects) are captured in `Workflow.Result`:
- No worker crash
- Error details in result for debugging
- Can still be resumed if `SuspendOnFailure` is set

---

## 4. Typed Execution IDs

### 4.1 Deterministic ID Generation

Replace ad-hoc `generateBatchId` with idempotency-key derivation:

```typescript
// Domain/Identity.ts

import { Hash } from "effect"

/** Derive deterministic BatchId from request content */
export const deriveIdempotentBatchId = (request: BatchRequest): Effect<BatchId> =>
  Effect.sync(() => {
    // Hash request content for idempotency
    const contentHash = Hash.string(JSON.stringify({
      ontologyVersion: request.ontologyVersion,
      documents: request.documents.map(d => d.sourceUri)
    }))

    // Format as batch-xxxxxxxxxxxx (12 hex chars)
    const hex = Math.abs(contentHash).toString(16).padStart(12, '0').slice(0, 12)
    return `batch-${hex}` as BatchId
  })
```

### 4.2 Idempotency Guarantee

Same request → same `batchId` → same `executionId` → workflow deduped by `@effect/workflow`

---

## 5. State Emission for SSE

### 5.1 Polling-Based State Stream

Since `WorkflowOrchestrator` lacks `subscribeToStateChanges`, use `Stream.repeatEffectOption`:

```typescript
// Service/BatchStateStream.ts

import { Stream, Effect, Option, Schedule } from "effect"
import { Workflow } from "@effect/workflow"

export const streamBatchState = (executionId: string): Stream<BatchState, string> =>
  Stream.repeatEffectOption(
    Effect.gen(function* () {
      const orchestrator = yield* WorkflowOrchestrator
      const result = yield* orchestrator.poll(executionId)

      if (result === undefined) {
        // Workflow not yet started or unknown
        return Option.none()
      }

      // Handle Workflow.Result variants
      if (result._tag === "Complete") {
        // Emit final state and end stream
        return Option.some(result.value)
      }

      if (result._tag === "Suspended") {
        // Workflow suspended - emit failure state with resumable flag
        const failedState: BatchFailed = {
          _tag: "Failed",
          batchId: /* extract from execution context */,
          failedAt: DateTime.unsafeNow(),
          failedInStage: result.lastStage ?? "pending",
          error: {
            code: "WORKFLOW_SUSPENDED",
            message: result.cause ?? "Workflow suspended",
            cause: undefined
          },
          lastSuccessfulStage: result.lastStage
        }
        return Option.some(failedState)
      }

      // In-progress - return current state from persistence
      // This requires intermediate state storage (see 5.2)
      const currentState = yield* getBatchStateFromStore(executionId)
      return Option.some(currentState)
    }).pipe(
      Effect.catchAll(() => Effect.succeed(Option.none()))
    )
  ).pipe(
    // Poll every 500ms
    Stream.schedule(Schedule.spaced("500 millis")),
    // Dedupe consecutive identical states
    Stream.changes,
    // Stop when terminal
    Stream.takeUntil(state => isTerminal(state))
  )
```

### 5.2 Intermediate State Persistence

Store state snapshots at each stage transition:

```typescript
// Inside workflow execution, after each stage:

const persistState = (state: BatchState) =>
  Effect.gen(function* () {
    const storage = yield* StorageService
    const key = `batches/${state.batchId}/state.json`
    yield* storage.set(key, JSON.stringify(Schema.encodeSync(BatchState)(state)))
  })

// Usage in workflow:
yield* extractionStage.pipe(
  Effect.tap((result) => persistState({
    _tag: "Extracting",
    batchId,
    documentsCompleted: result.completed,
    documentsTotal: result.total,
    // ...
  }))
)
```

---

## 6. SSE Implementation

### 6.1 Using @effect/experimental/Sse

```typescript
// Http/Routes/Extract.ts

import { Sse } from "@effect/experimental"
import { HttpServerRequest, HttpServerResponse } from "@effect/platform"
import { Stream, Chunk, Effect } from "effect"

const batchStateToSseEvent = (state: BatchState): Sse.Event => ({
  _tag: "Event",
  event: "state",
  id: `${state.batchId}-${state._tag}`,
  data: JSON.stringify(Schema.encodeSync(BatchState)(state))
})

export const streamBatchResponse = (executionId: string) =>
  Effect.gen(function* () {
    const stateStream = streamBatchState(executionId)

    // Convert to SSE events and encode
    const sseStream = stateStream.pipe(
      Stream.map(batchStateToSseEvent),
      Stream.map(event => Sse.encoder.write(event)),
      Stream.encodeText
    )

    return HttpServerResponse.stream(sseStream, {
      headers: {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive"
      }
    })
  })
```

### 6.2 SSE Event Format

```
event: state
id: batch-abc123-Pending
data: {"_tag":"Pending","batchId":"batch-abc123",...}

event: state
id: batch-abc123-Extracting
data: {"_tag":"Extracting","batchId":"batch-abc123","documentsCompleted":1,"documentsTotal":3,...}

event: state
id: batch-abc123-Complete
data: {"_tag":"Complete","batchId":"batch-abc123","stats":{...},...}
```

---

## 7. Compensation & Cleanup

### 7.1 Wrap Storage Writes

```typescript
// Inside workflow execution:

const writeExtractionOutput = (batchId: BatchId, graphs: Array<Graph>) =>
  Workflow.withCompensation(
    // Main effect: write extraction output
    Effect.gen(function* () {
      const storage = yield* StorageService
      const outputPath = `batches/${batchId}/extraction-output.json`
      yield* storage.set(outputPath, JSON.stringify(graphs))
      return `gs://${bucket}/${outputPath}` as GcsUri
    }),
    // Compensation: delete on workflow failure
    (outputUri, cause) =>
      Effect.gen(function* () {
        const storage = yield* StorageService
        yield* storage.remove(stripGsPrefix(outputUri))
        yield* Effect.logWarning(`Cleaned up ${outputUri} due to workflow failure`, { cause })
      })
  )
```

### 7.2 Compensation Chain

Each stage wraps artifacts with compensation:
1. **Extraction** → Clean extraction-output.json
2. **Resolution** → Clean resolved-graph.ttl
3. **Validation** → Clean validated-graph.ttl

Ingestion (final stage) doesn't need compensation - if it fails, validated artifacts remain for retry.

---

## 8. Configurable Concurrency

### 8.1 Config Schema Update

```typescript
// Service/Config.ts (existing RuntimeConfig)

const RuntimeConfig = Config.nested("RUNTIME")(Config.all({
  concurrency: Config.integer("CONCURRENCY").pipe(Config.withDefault(4)),
  llmConcurrencyLimit: Config.integer("LLM_CONCURRENCY").pipe(Config.withDefault(2)),
  extractionConcurrency: Config.integer("EXTRACTION_CONCURRENCY").pipe(Config.withDefault(5)),
  resolutionConcurrency: Config.integer("RESOLUTION_CONCURRENCY").pipe(Config.withDefault(10)),
  // ... existing fields
}))
```

### 8.2 Usage in Workflow

```typescript
// Replace hardcoded values:
const config = yield* ConfigService

yield* Effect.forEach(
  documents,
  extractDocument,
  { concurrency: config.runtime.extractionConcurrency }
)
```

### 8.3 Progress Emission per Document

Emit progress after each document (not just batch completion):

```typescript
yield* Effect.forEach(
  documents,
  (doc, index) => extractDocument(doc).pipe(
    Effect.tap(() => persistState({
      _tag: "Extracting",
      documentsCompleted: index + 1,
      documentsTotal: documents.length,
      currentDocumentId: doc.documentId,
      // ...
    }))
  ),
  { concurrency: config.runtime.extractionConcurrency }
)
```

---

## 9. HTTP Surface

### 9.1 Route Definitions

```typescript
// Http/Routes/Extract.ts

import { HttpServerRequest, HttpServerResponse } from "@effect/platform"

// POST /v1/extract/batch - Start batch extraction
export const batchExtractRoute = HttpRouter.post(
  "/v1/extract/batch",
  Effect.gen(function* () {
    // Decode request body with schema
    const request = yield* HttpServerRequest.schemaBodyJson(BatchRequest)

    // Start extraction (ingress + workflow)
    const { batchId, executionId, manifestUri } = yield* startBatchExtraction(request)

    // Return SSE stream
    return yield* streamBatchResponse(executionId)
  })
)

// POST /v1/extract - Single document convenience
export const singleExtractRoute = HttpRouter.post(
  "/v1/extract",
  Effect.gen(function* () {
    const doc = yield* HttpServerRequest.schemaBodyJson(RequestDocument)

    // Wrap in batch request
    const batchRequest: BatchRequest = {
      ontologyUri: /* from config or header */,
      ontologyVersion: /* from config or header */,
      targetNamespace: /* from config or header */,
      documents: [doc]
    }

    const { executionId } = yield* startBatchExtraction(batchRequest)
    return yield* streamBatchResponse(executionId)
  })
)

// GET /v1/batch/:id - Query batch state
export const batchStatusRoute = HttpRouter.get(
  "/v1/batch/:id",
  Effect.gen(function* () {
    const { id } = yield* HttpRouter.params
    const batchId = Schema.decodeSync(BatchId)(id)

    const orchestrator = yield* WorkflowOrchestrator
    const result = yield* orchestrator.poll(batchId)

    if (result === undefined) {
      return HttpServerResponse.json({ error: "Batch not found" }, { status: 404 })
    }

    // Map Workflow.Result to BatchState
    const state = result._tag === "Complete"
      ? result.value
      : yield* getBatchStateFromStore(batchId)

    return HttpServerResponse.schemaJson(BatchState)(state)
  })
)
```

### 9.2 Single-Doc Convenience Headers

For single-doc endpoint, derive ontology config from headers:

```typescript
const singleExtractRoute = HttpRouter.post(
  "/v1/extract",
  Effect.gen(function* () {
    const headers = yield* HttpServerRequest.headers
    const doc = yield* HttpServerRequest.schemaBodyJson(RequestDocument)

    // Extract config from headers or use defaults
    const ontologyUri = headers["x-ontology-uri"] ?? defaultOntologyUri
    const ontologyVersion = headers["x-ontology-version"] ?? defaultOntologyVersion
    const targetNamespace = headers["x-target-namespace"] ?? defaultNamespace

    const batchRequest: BatchRequest = {
      ontologyUri: ontologyUri as GcsUri,
      ontologyVersion: ontologyVersion as OntologyVersion,
      targetNamespace: targetNamespace as Namespace,
      documents: [doc]
    }

    const { executionId } = yield* startBatchExtraction(batchRequest)
    return yield* streamBatchResponse(executionId)
  })
)
```

---

## 10. Layer Composition & Testing

### 10.1 Production Layer

```typescript
// Runtime/ProductionRuntime.ts

export const ServerLayer = Layer.mergeAll(
  // Core services
  ConfigService.Live,
  StorageService.GcsLive,

  // Workflow with PostgreSQL persistence
  WorkflowOrchestrator.Live,
  BatchExtractionWorkflow.layer,
  PostgresPersistenceLive,

  // HTTP server
  BunHttpServer.layer({ port: 8080 }),

  // Extraction services
  ExtractionService.Live,
  NlpService.Live
)
```

### 10.2 Test Layer (Memory Persistence)

```typescript
// Runtime/TestRuntime.ts

import { WorkflowEngine } from "@effect/workflow"

export const TestServerLayer = Layer.mergeAll(
  ConfigService.Test,
  StorageService.MemoryLive,

  // Use in-memory workflow engine for tests
  WorkflowOrchestrator.Live,
  BatchExtractionWorkflow.layer,
  WorkflowEngine.layerMemory,  // No PostgreSQL needed

  BunHttpServer.layer({ port: 0 }),  // Random port

  ExtractionService.Mock,
  NlpService.Mock
)
```

### 10.3 Smoke Test

```typescript
// test/integration/batch-workflow.test.ts

import { it } from "@effect/vitest"

it.effect("processes single document batch", () =>
  Effect.gen(function* () {
    const request: BatchRequest = {
      ontologyUri: "gs://test-bucket/ontology.ttl" as GcsUri,
      ontologyVersion: "test/ontology@abc123" as OntologyVersion,
      targetNamespace: "https://test.org/" as Namespace,
      documents: [{
        sourceUri: "gs://test-bucket/doc.txt" as GcsUri,
        contentType: "text/plain"
      }]
    }

    const { batchId, executionId } = yield* startBatchExtraction(request)

    // Wait for completion
    const orchestrator = yield* WorkflowOrchestrator
    const result = yield* orchestrator.startAndWait({ batchId, manifestUri, ontologyVersion })

    expect(result._tag).toBe("Complete")
    expect(result.stats.documentsProcessed).toBe(1)
  }).pipe(Effect.provide(TestServerLayer))
)
```

---

## 11. Implementation Checklist

### Phase 1: Schema & Ingress (Day 1)
- [ ] Create `Domain/Schema/BatchRequest.ts`
- [ ] Implement `Service/BatchIngress.ts` with `createManifest`, `stageManifest`
- [ ] Add `deriveIdempotentBatchId` to `Domain/Identity.ts`
- [ ] Update `ConfigService` with `extractionConcurrency`, `resolutionConcurrency`

### Phase 2: Workflow Annotations (Day 1)
- [ ] Add `SuspendOnFailure` and `CaptureDefects` annotations
- [ ] Implement `Workflow.withCompensation` for storage writes
- [ ] Update concurrency to use config values

### Phase 3: State Persistence (Day 2)
- [ ] Add `persistState` calls at each workflow stage
- [ ] Implement `getBatchStateFromStore`
- [ ] Create `Service/BatchStateStream.ts` with polling logic

### Phase 4: SSE Endpoints (Day 2)
- [ ] Add `@effect/experimental` dependency
- [ ] Implement `streamBatchResponse` with `Sse.encoder`
- [ ] Add `POST /v1/extract/batch` route
- [ ] Add `POST /v1/extract` single-doc route
- [ ] Add `GET /v1/batch/:id` status route

### Phase 5: Cleanup (Day 3)
- [ ] Delete `JobManager.ts`
- [ ] Remove `/v1/jobs/*` routes
- [ ] Update server.ts layer composition
- [ ] Add smoke tests with `WorkflowEngine.layerMemory`

### Phase 6: Deploy & Verify (Day 3)
- [ ] Deploy to dev environment
- [ ] Verify PostgreSQL connectivity
- [ ] Test SSE streaming end-to-end
- [ ] Test workflow suspension/resume

---

## 12. API Reference

### Endpoints

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/v1/extract/batch` | Start batch extraction, returns SSE stream |
| `POST` | `/v1/extract` | Single-doc convenience, returns SSE stream |
| `GET` | `/v1/batch/:id` | Query batch state |

### Request: POST /v1/extract/batch

```typescript
{
  "ontologyUri": "gs://bucket/ontology.ttl",
  "ontologyVersion": "football/ontology@a1b2c3d4",
  "targetNamespace": "https://example.org/kg/",
  "documents": [
    {
      "sourceUri": "gs://bucket/docs/article.txt",
      "contentType": "text/plain"
    }
  ]
}
```

### Response: SSE Stream

```
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive

event: state
id: batch-abc123-Pending
data: {"_tag":"Pending","batchId":"batch-abc123","documentCount":1,...}

event: state
id: batch-abc123-Extracting
data: {"_tag":"Extracting","batchId":"batch-abc123","documentsCompleted":0,"documentsTotal":1,...}

event: state
id: batch-abc123-Complete
data: {"_tag":"Complete","batchId":"batch-abc123","stats":{"documentsProcessed":1,...},...}
```

### Response: GET /v1/batch/:id

```typescript
{
  "_tag": "Extracting",
  "batchId": "batch-abc123",
  "manifestUri": "gs://bucket/batches/batch-abc123/manifest.json",
  "ontologyVersion": "football/ontology@a1b2c3d4",
  "documentsTotal": 3,
  "documentsCompleted": 1,
  "documentsFailed": 0,
  "currentDocumentId": "doc-xyz789",
  "createdAt": "2025-12-11T10:00:00.000Z",
  "updatedAt": "2025-12-11T10:01:30.000Z"
}
```

---

## 13. Migration Notes

### Breaking Changes

1. **No `/v1/jobs/*` endpoints** - Use `/v1/batch/:id` instead
2. **SSE instead of polling** - Clients must handle `text/event-stream`
3. **Request schema changed** - `batchId`/`createdAt` now server-generated

### Client Update Guide

```typescript
// Before (v1 - never deployed)
const response = await fetch('/v1/jobs', { method: 'POST', body: {...} })
const { jobId } = await response.json()
// Poll: GET /v1/jobs/{jobId}

// After (v2)
const response = await fetch('/v1/extract/batch', {
  method: 'POST',
  body: JSON.stringify({...}),
  headers: { 'Accept': 'text/event-stream' }
})

const reader = response.body.getReader()
const decoder = new TextDecoder()

while (true) {
  const { done, value } = await reader.read()
  if (done) break

  const text = decoder.decode(value)
  // Parse SSE events
  for (const line of text.split('\n')) {
    if (line.startsWith('data: ')) {
      const state = JSON.parse(line.slice(6))
      console.log(state._tag, state)
    }
  }
}
```

---

## Review Notes (v2 Effect deep dive)

- **Handle `Workflow.Result` correctly**: `WorkflowEngine.poll` returns `Complete | Suspended`, where `Complete.exit` is an `Exit` (succeed/fail/die). Decode with `Exit.matchEffect` and surface suspension causes via `Suspended.cause`; the current examples that read `result.value` would silently drop failures/defects even with `CaptureDefects` on by default. Consider `Workflow.intoResult` to simplify mapping to `BatchState`.
- **Reuse the workflow’s own execution ID**: instead of a bespoke `deriveIdempotentBatchId`, call `BatchExtractionWorkflow.executionId(payload)` (it derives from `idempotencyKey`). If you keep the hash, include all inputs that change behavior (`ontologyUri`, `ontologyVersion`, `targetNamespace`, optional `shaclUri`, document IDs) to avoid collisions across semantically different batches.
- **SSE stream shaping**: `Stream.changes` uses reference equality, so polling will still emit on every tick. Use `Stream.changesWith((a, b) => a._tag === b._tag && a.updatedAt.getTime() === b.updatedAt.getTime())` (or compare encoded JSON) and interrupt the stream on `HttpServerRequest.signal` to stop polling when the client disconnects. Add periodic keep-alives via `Sse.encoder.write(new Sse.Retry(Duration.seconds(15)))` to keep long runs alive behind proxies.
- **State persistence alignment**: rather than ad-hoc GCS writes, consider storing `BatchState` via `@effect/experimental/Persistence.layerKeyValueStore` (you already adapter-wrap `StorageService` in `WorkflowPersistence`), then stream states with `Stream.asyncScoped`/`Reactivity.stream` fed by a hub/ref so SSE and `GET` share the same source of truth.
- **Compensation scope**: `Workflow.withCompensation` only registers finalizers for top-level workflow effects, not for nested `Activity.make` bodies. Most artifacts are written inside activities today, so add cleanups there (e.g., wrap `storage.set` in the activity or add explicit cleanup activities) or move the writes to the workflow and wrap them with `withCompensation`.
- **Suspension policy**: `CaptureDefects` defaults to `true`; keep `SuspendOnFailure` but also consider `suspendedRetrySchedule` on `Workflow.make` to automatically retry/resume (e.g., spaced backoff) instead of requiring a manual `resume` call.
- **Ingress fidelity**: `Schema.encodeSync` will throw on invalid manifests—prefer `Schema.encodeEffect` and map to `HttpServerResponse.schemaJsonError` for 4xx. When `sizeBytes` is missing, populate from `getUint8Array` length to keep downstream stats accurate.
- **Endpoint typing**: align `batchStatusRoute` with `Workflow.Result` semantics above, and return 404 vs. `Suspended` vs. failed `Exit` distinctly. Use `HttpServerResponse.schemaJson(BatchState)` so Date fields are ISO-encoded consistently with the `Schema.DateTimeUtc` encoder.
