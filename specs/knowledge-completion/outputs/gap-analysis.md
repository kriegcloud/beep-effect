# Gap Analysis

> Generated by: Orchestrator after Tasks 1-3 in Phase 1
> Status: **COMPLETE**

---

## Executive Summary

The migration from custom `AiService` to `@effect/ai` is **straightforward** with **low risk**. The critical finding is that all 5 LLM calls use `generateObjectWithSystem`, which maps cleanly to `Prompt.make()` + `generateObject()` in @effect/ai.

**Migration Scope**:
- 3 files to modify (extractors)
- 2 files to delete (AiService + index)
- 2-3 files to create (layers, retry wrapper)
- 0 provider implementations to migrate (none exist)

---

## Files to Modify

| File | Change Type | Complexity | Notes |
|------|-------------|------------|-------|
| `Extraction/EntityExtractor.ts` | Modify | Medium | 1 `generateObjectWithSystem` call → `Prompt.make()` |
| `Extraction/MentionExtractor.ts` | Modify | Medium | 2 `generateObjectWithSystem` calls → `Prompt.make()` |
| `Extraction/RelationExtractor.ts` | Modify | Medium | 2 `generateObjectWithSystem` calls → `Prompt.make()` |
| `Extraction/ExtractionPipeline.ts` | Modify | Low | Update dependencies, remove AiService |
| `Ai/PromptTemplates.ts` | Modify | Low | Optional: Convert to `Prompt.make()` format |

### Migration Order (Dependency-Safe)

1. **Create** `Runtime/LlmLayers.ts` - Provider layers
2. **Create** `Service/LlmWithRetry.ts` - Retry wrapper (optional)
3. **Modify** `MentionExtractor.ts` - First extractor
4. **Modify** `EntityExtractor.ts` - Second extractor
5. **Modify** `RelationExtractor.ts` - Third extractor
6. **Modify** `ExtractionPipeline.ts` - Update dependencies
7. **Delete** `Ai/AiService.ts` - After all migrations verified
8. **Update** `Ai/index.ts` - Remove AiService exports

---

## Files to Delete

| File | Reason | Blocker |
|------|--------|---------|
| `Ai/AiService.ts` | Replaced by `LanguageModel.LanguageModel` | Wait until all extractors migrated |
| `Ai/index.ts` | Re-exports AiService | Update exports first |

**Note**: `PromptTemplates.ts` can be **kept** - the template functions can return strings that are used with `Prompt.make()`.

---

## Files to Create

| File | Purpose | Priority |
|------|---------|----------|
| `Runtime/LlmLayers.ts` | Provider layer composition (Anthropic/OpenAI) | P0 |
| `Service/LlmWithRetry.ts` | Retry wrapper with telemetry | P1 |
| `test/_shared/MockLlmLayer.ts` | Mock LanguageModel for tests | P0 |

### LlmLayers.ts Skeleton

```typescript
import { AnthropicLanguageModel, AnthropicClient } from "@effect/ai-anthropic"
import { OpenAiLanguageModel, OpenAiClient } from "@effect/ai-openai"
import { FetchHttpClient } from "@effect/platform"
import { Layer, Config, Effect } from "effect"

// Configuration service (or use @beep/env)
export const LlmProviderLayer = Layer.unwrapEffect(
  Effect.gen(function*() {
    const provider = yield* Config.string("LLM_PROVIDER").pipe(
      Config.withDefault("anthropic")
    )
    const apiKey = yield* Config.string("LLM_API_KEY")
    const model = yield* Config.string("LLM_MODEL").pipe(
      Config.withDefault("claude-sonnet-4-20250514")
    )

    switch (provider) {
      case "anthropic":
        return AnthropicLanguageModel.layer({ model }).pipe(
          Layer.provide(AnthropicClient.layer({ apiKey })),
          Layer.provide(FetchHttpClient.layer)
        )
      case "openai":
        return OpenAiLanguageModel.layer({ model }).pipe(
          Layer.provide(OpenAiClient.layer({ apiKey })),
          Layer.provide(FetchHttpClient.layer)
        )
      default:
        throw new Error(`Unknown LLM provider: ${provider}`)
    }
  })
)
```

---

## Migration Complexity Assessment

### Per-File Complexity

| File | Complexity | Reasoning |
|------|------------|-----------|
| EntityExtractor.ts | **Medium** | 1 call, needs Prompt.make() refactor |
| MentionExtractor.ts | **Medium** | 2 calls, batch pattern |
| RelationExtractor.ts | **Medium** | 2 calls, batch pattern |
| ExtractionPipeline.ts | **Low** | Dependencies update only |
| LlmLayers.ts (new) | **Medium** | Config-driven provider selection |
| LlmWithRetry.ts (new) | **Medium** | Copy from reference, adapt |
| MockLlmLayer.ts (new) | **Low** | Simple mock |

### Overall Assessment

| Metric | Value |
|--------|-------|
| **Total Files** | ~10 files |
| **Lines Changed** | ~300-500 LOC |
| **Risk Level** | **LOW** |
| **Estimated Effort** | 1-2 sessions |

---

## Dependencies to Add

Add to `packages/knowledge/server/package.json`:

```json
{
  "dependencies": {
    "@effect/ai": "^0.x.x",
    "@effect/ai-anthropic": "^0.x.x",
    "@effect/ai-openai": "^0.x.x"
  }
}
```

**Note**: Check latest versions in npm registry.

---

## Critical Questions Resolved

### 1. System Prompt Support

**Answer**: ✅ YES - via `Prompt.make()` with role-based messages

```typescript
// Migration pattern
const prompt = Prompt.make([
  { role: "system", content: systemPrompt },
  { role: "user", content: userPrompt }
])
yield* model.generateObject(prompt, schema, config)
```

### 2. Mock Layer Pattern

**Answer**: ✅ Use `LanguageModel.LanguageModel.of()`

```typescript
const MockLlmLayer = Layer.succeed(
  LanguageModel.LanguageModel,
  LanguageModel.LanguageModel.of({
    generate: () => Stream.empty,
    generateObject: () => Effect.succeed(mockData)
  })
)
```

### 3. Provider Selection Pattern

**Answer**: ✅ Use `Layer.unwrapEffect` with config-based switch

```typescript
const ProviderLayer = Layer.unwrapEffect(
  Effect.gen(function*() {
    const provider = yield* Config.string("LLM_PROVIDER")
    switch (provider) {
      case "anthropic": return AnthropicLayer
      case "openai": return OpenAILayer
    }
  })
)
```

---

## Migration Transformation

### Before (Current)

```typescript
// EntityExtractor.ts (current)
const ai = yield* AiService
const result = yield* ai.generateObjectWithSystem(
  EntityOutput,           // schema
  buildSystemPrompt(),    // system prompt
  buildEntityPrompt(text), // user prompt
  config
)
```

### After (Target)

```typescript
// EntityExtractor.ts (migrated)
import { LanguageModel, Prompt } from "@effect/ai"

const model = yield* LanguageModel.LanguageModel
const prompt = Prompt.make([
  { role: "system", content: buildSystemPrompt() },
  { role: "user", content: buildEntityPrompt(text) }
])
const result = yield* model.generateObject(prompt, EntityOutput, config)
```

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Type errors during migration | Medium | Low | Run `bun run check` after each file |
| Missing provider config | Medium | Medium | Add config validation early |
| Test failures | Low | Low | Create mock layer first |
| Schema compatibility | Low | Low | Effect Schema unchanged |

---

## Recommended Migration Strategy

### Phase 4 Execution Plan

1. **Setup** (30 min)
   - Add dependencies
   - Create `LlmLayers.ts`
   - Create `MockLlmLayer.ts`

2. **Migrate Extractors** (1-2 hours)
   - MentionExtractor first (simpler)
   - EntityExtractor second
   - RelationExtractor last
   - Run `bun run check` after each

3. **Update Pipeline** (15 min)
   - Update ExtractionPipeline dependencies
   - Remove AiService from layer composition

4. **Cleanup** (15 min)
   - Delete AiService.ts
   - Update Ai/index.ts exports
   - Run final verification

5. **Verification** (15 min)
   - `bun run check --filter @beep/knowledge-server`
   - `bun run lint:fix --filter @beep/knowledge-server`

---

## Summary

| Item | Status |
|------|--------|
| System prompt support | ✅ Resolved |
| Mock layer pattern | ✅ Resolved |
| Provider selection | ✅ Resolved |
| Migration complexity | **LOW** |
| Estimated effort | 1-2 sessions |
| Risk level | **LOW** |
| Files to modify | 5 |
| Files to delete | 2 |
| Files to create | 3 |
