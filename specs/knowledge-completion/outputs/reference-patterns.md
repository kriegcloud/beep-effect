# Reference Implementation Patterns

> Generated by: `codebase-researcher` agent in Phase 1
> Status: **COMPLETE**

---

## Summary

The effect-ontology reference implementation demonstrates mature patterns for using `@effect/ai` with:
- System prompt handling via `Prompt.fromMessages`
- Structured retry logic with feedback
- Dynamic provider layer composition
- Comprehensive test mocking strategies

---

## Relevant Files

| File | Purpose |
|------|---------|
| `tmp/effect-ontology/packages/@core-v2/src/Service/Extraction.ts` | Entity/Relation extraction using LanguageModel |
| `tmp/effect-ontology/packages/@core-v2/src/Service/LlmWithRetry.ts` | Retry wrapper with telemetry |
| `tmp/effect-ontology/packages/@core-v2/src/Service/GenerateWithFeedback.ts` | Feedback-based retry for schema errors |
| `tmp/effect-ontology/packages/@core-v2/src/Service/PromptCache.ts` | System prompt construction with caching |
| `tmp/effect-ontology/packages/@core-v2/src/Runtime/ProductionRuntime.ts` | Provider layer composition |
| `tmp/effect-ontology/packages/@core-v2/src/Runtime/TestRuntime.ts` | Mock LanguageModel layer |
| `tmp/effect-ontology/packages/@core-v2/src/Runtime/RateLimitedLanguageModel.ts` | Rate-limited LanguageModel wrapper |

---

## Pattern 1: LanguageModel Usage

**Location**: `src/Service/Extraction.ts:66-68`

```typescript
import { LanguageModel } from "@effect/ai"

const llm = yield* LanguageModel.LanguageModel

const response = yield* llm.generateObject({
  prompt: promptObj,
  schema,
  objectName: "EntityGraph"
})
```

---

## Pattern 2: System Prompt Construction (CRITICAL)

**Location**: `src/Service/PromptCache.ts:36-55`

```typescript
import { Prompt } from "@effect/ai"

export const makeCachedPrompt = (
  systemMessage: string,
  userMessage: string,
  _enableCaching: boolean
): Prompt.Prompt => {
  return Prompt.fromMessages([
    Prompt.makeMessage("system", {
      content: systemMessage
    }),
    Prompt.makeMessage("user", {
      content: [Prompt.makePart("text", { text: userMessage })]
    })
  ])
}
```

### StructuredPrompt Interface

```typescript
export interface StructuredPrompt {
  readonly systemMessage: string  // Cacheable rules, schema, instructions
  readonly userMessage: string    // Variable input text
}
```

---

## Pattern 3: generateObjectWithRetry

**Location**: `src/Service/LlmWithRetry.ts:67-167`

```typescript
export interface GenerateObjectWithRetryOptions<A, I, R> {
  readonly llm: LanguageModel.Service
  readonly prompt: string | StructuredPrompt
  readonly schema: Schema.Schema<A, I, R>
  readonly objectName: string
  readonly serviceName: string
  readonly retryConfig: RetryConfig
}

export const generateObjectWithRetry = <A, I, R>(
  options: GenerateObjectWithRetryOptions<A, I, R>
) =>
  Effect.gen(function*() {
    const promptObj: Prompt.Prompt = typeof prompt === "string"
      ? Prompt.make(prompt)
      : makeCachedPromptFromStructured(prompt, enablePromptCaching)

    return yield* llm.generateObject({
      prompt: promptObj,
      schema,
      objectName
    }).pipe(
      Effect.timeout(Duration.millis(retryConfig.timeoutMs)),
      Effect.retry({ schedule: retryPolicy }),
      Effect.tapErrorCause((cause) =>
        Effect.logError(`${serviceName} LLM call failed`, { cause })
      ),
      Effect.tap((response) =>
        annotateLlmCall({
          inputTokens: response.usage.inputTokens,
          outputTokens: response.usage.outputTokens
        })
      ),
      Effect.withSpan(`${serviceName.toLowerCase()}-llm`)
    )
  })
```

---

## Pattern 4: Provider Layer Composition

**Location**: `src/Runtime/ProductionRuntime.ts:65-113`

```typescript
import { AnthropicClient, AnthropicLanguageModel } from "@effect/ai-anthropic"
import { OpenAiClient, OpenAiLanguageModel } from "@effect/ai-openai"
import { FetchHttpClient } from "@effect/platform"

export const makeLanguageModelLayer = Layer.unwrapEffect(
  Effect.gen(function*() {
    const config = yield* ConfigService

    switch (config.llm.provider) {
      case "anthropic": {
        return AnthropicLanguageModel.layer({ model: config.llm.model }).pipe(
          Layer.provide(
            AnthropicClient.layer({ apiKey: config.llm.apiKey }).pipe(
              Layer.provide(FetchHttpClient.layer)
            )
          )
        )
      }

      case "openai": {
        return OpenAiLanguageModel.layer({ model: config.llm.model }).pipe(
          Layer.provide(
            OpenAiClient.layer({ apiKey: config.llm.apiKey }).pipe(
              Layer.provide(FetchHttpClient.layer)
            )
          )
        )
      }

      default: {
        return AnthropicLanguageModel.layer({ model: config.llm.model }).pipe(
          Layer.provide(
            AnthropicClient.layer({ apiKey: config.llm.apiKey }).pipe(
              Layer.provide(FetchHttpClient.layer)
            )
          )
        )
      }
    }
  })
)
```

---

## Pattern 5: Rate-Limited LanguageModel Wrapper

**Location**: `src/Runtime/RateLimitedLanguageModel.ts:67-190`

```typescript
export const RateLimitedLanguageModelLayer = Layer.scoped(
  LanguageModel.LanguageModel,
  Effect.gen(function*() {
    const baseLlm = yield* LanguageModel.LanguageModel
    const scope = yield* Scope.Scope

    const perSecondLimiter = yield* RateLimiter.make({
      limit: 2,
      interval: "1 seconds",
      algorithm: "fixed-window"
    }).pipe(Scope.extend(scope))

    const perMinuteLimiter = yield* RateLimiter.make({
      limit: 20,
      interval: "1 minutes",
      algorithm: "fixed-window"
    }).pipe(Scope.extend(scope))

    const rateLimiter = <A, E, R>(effect: Effect.Effect<A, E, R>) =>
      perSecondLimiter(perMinuteLimiter(effect))

    const circuitBreaker = yield* makeCircuitBreaker({
      maxFailures: 5,
      resetTimeout: Duration.minutes(2)
    }).pipe(Scope.extend(scope))

    return LanguageModel.LanguageModel.of({
      generateObject: (opts) =>
        circuitBreaker.protect(rateLimiter(baseLlm.generateObject(opts))).pipe(
          Effect.withSpan("llm.generateObject")
        )
    })
  })
)
```

---

## Pattern 6: Mock LanguageModel for Testing

**Location**: `src/Runtime/TestRuntime.ts:49-62`

```typescript
const MockLanguageModel = Layer.succeed(
  LanguageModel.LanguageModel,
  LanguageModel.LanguageModel.of({
    generateText: () => Effect.succeed(new LanguageModel.GenerateTextResponse<{}>([])),
    streamText: () => Stream.fromIterable([]),
    generateObject: () =>
      Effect.succeed(
        new LanguageModel.GenerateObjectResponse<{}, any>(
          { entities: [], relations: [] },
          []
        )
      )
  })
)
```

---

## Pattern 7: Parameterized Test Mocks

```typescript
const createMockLlm = (response: {
  strategy: CorrectionStrategy
  newValue?: string | number
  explanation: string
  confidence: number
}) =>
  Layer.succeed(LanguageModel.LanguageModel, {
    generateObject: () =>
      Effect.succeed({
        value: response,
        usage: {
          inputTokens: 100,
          outputTokens: 50,
          totalTokens: 150
        }
      } as any),
    generateText: () => Effect.succeed({
      text: "",
      usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 }
    } as any),
    streamText: () => Stream.empty
  } as unknown as LanguageModel.Service)

// Usage in tests:
Effect.provide(createMockLlm({
  strategy: "generate-value",
  newValue: "alice@example.com",
  explanation: "Generated email",
  confidence: 0.9
}))
```

---

## Differences from Current AiService

| Aspect | Current AiService | effect-ontology Pattern |
|--------|-------------------|-------------------------|
| **Service Access** | Custom `AiService` tag | Direct `LanguageModel.LanguageModel` |
| **System Prompts** | `generateObjectWithSystem` method | `Prompt.fromMessages([...])` |
| **Provider Selection** | No provider | `Layer.unwrapEffect` with config-based switch |
| **Retry Logic** | Not implemented | `generateObjectWithRetry` wrapper |
| **Rate Limiting** | Not implemented | `RateLimitedLanguageModelLayer` |
| **Test Mocking** | Mock fails with error | `Layer.succeed(LanguageModel.LanguageModel, {...})` |

---

## Critical Finding: System Prompt Migration

The key pattern for EntityExtractor migration:

```typescript
// 1. Define StructuredPrompt
interface StructuredPrompt {
  readonly systemMessage: string
  readonly userMessage: string
}

// 2. Convert to @effect/ai Prompt
const prompt = Prompt.fromMessages([
  Prompt.makeMessage("system", { content: systemMessage }),
  Prompt.makeMessage("user", { content: [Prompt.makePart("text", { text: userMessage })] })
])

// 3. Call LLM with schema
const response = yield* llm.generateObject({
  prompt,
  schema: MySchema,
  objectName: "MyOutput"
})
```

---

## Recommendations

### Patterns to Follow

1. **Use `Prompt.fromMessages` for system prompts**
2. **Use `Layer.unwrapEffect` for dynamic provider selection**
3. **Create dedicated retry wrappers with telemetry**
4. **Use `LanguageModel.LanguageModel.of({...})` for mocks**

### Patterns to Avoid

1. **Avoid string-only prompts** when system context is needed
2. **Avoid hardcoded provider selection** - use config-driven layers
3. **Avoid wrapping LanguageModel** unless adding cross-cutting concerns
