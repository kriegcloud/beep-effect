# @effect/ai Research

> Generated by: `effect-researcher` agent in Phase 1
> Status: **COMPLETE**

---

## Executive Summary

The `@effect/ai` package provides a unified, Effect-based abstraction for Large Language Models (LLMs). It supports multiple providers (Anthropic, OpenAI) through a common `LanguageModel` service interface and provides type-safe prompt construction with `Prompt.make()` and structured output generation with `generateObject()`.

**CRITICAL FINDINGS**:
1. ✅ System prompts ARE supported via `Prompt.make()` with role-based messages
2. ✅ Mock layers use `LanguageModel.LanguageModel.of()` for test doubles
3. ✅ `generateObject` accepts both string prompts and structured `Prompt` objects
4. ⚠️ No `generateObjectWithSystem` - use `Prompt.make()` instead

---

## LanguageModel Service

### Service Interface

```typescript
import * as LanguageModel from "@effect/ai/LanguageModel"

interface LanguageModel {
  readonly generate: (
    request: LanguageModelRequest.Request
  ) => Stream.Stream<LanguageModelResponse.Success, LanguageModelResponse.Failure>

  readonly generateObject: <A, I, R>(
    prompt: string | Prompt.Prompt,
    schema: Schema.Schema<A, I, R>,
    options?: LanguageModelRequest.ObjectOptions
  ) => Effect.Effect<A, LanguageModelResponse.Failure, R>
}
```

### Service Access

```typescript
const program = Effect.gen(function*() {
  const model = yield* LanguageModel.LanguageModel
  // Use model...
})
```

---

## Prompt API

### `Prompt.make()` - Role-Based Messages

```typescript
import * as Prompt from "@effect/ai/Prompt"

// Simple user prompt
const userPrompt = Prompt.make("What is the capital of France?")

// WITH SYSTEM PROMPT (CRITICAL PATTERN)
const promptWithSystem = Prompt.make([
  {
    role: "system",
    content: "You are a helpful assistant that extracts entities from text."
  },
  {
    role: "user",
    content: "Extract entities from: John works at Google in San Francisco."
  }
])
```

### Message Roles

```typescript
type Message =
  | { role: "system"; content: string }
  | { role: "user"; content: string }
  | { role: "assistant"; content: string }
```

---

## generateObject API

### Function Signature

```typescript
function generateObject<A, I, R>(
  prompt: string | Prompt.Prompt,
  schema: Schema.Schema<A, I, R>,
  options?: {
    maxTokens?: number
    temperature?: number
    topP?: number
    frequencyPenalty?: number
    presencePenalty?: number
    stopSequences?: ReadonlyArray<string>
    seed?: number
  }
): Effect.Effect<A, LanguageModelResponse.Failure, R>
```

### Basic Usage

```typescript
const EntitySchema = S.Struct({
  name: S.String,
  type: S.Literal("person", "organization", "location"),
  confidence: S.Number
})

const extractEntities = (text: string) =>
  Effect.gen(function*() {
    const model = yield* LanguageModel.LanguageModel

    return yield* model.generateObject(
      `Extract entities from: ${text}`,
      S.Array(EntitySchema)
    )
  })
```

### WITH SYSTEM PROMPT (CRITICAL PATTERN)

```typescript
const extractEntitiesWithSystem = (text: string) =>
  Effect.gen(function*() {
    const model = yield* LanguageModel.LanguageModel

    const prompt = Prompt.make([
      {
        role: "system",
        content: "You are an expert entity extraction system."
      },
      {
        role: "user",
        content: `Extract entities from: ${text}`
      }
    ])

    return yield* model.generateObject(
      prompt,
      S.Array(EntitySchema),
      { temperature: 0.1 }
    )
  })
```

---

## System Prompt Support (CRITICAL)

### Answer: YES - Via `Prompt.make()`

```typescript
// CORRECT PATTERN
const withSystemPrompt = Effect.gen(function*() {
  const model = yield* LanguageModel.LanguageModel

  const prompt = Prompt.make([
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Hello!" }
  ])

  return yield* model.generateObject(prompt, schema)
})
```

### EntityExtractor Migration Solution

```typescript
// BEFORE (current custom API)
yield* ai.generateObjectWithSystem(
  schema,
  systemPrompt,
  userPrompt,
  config
)

// AFTER (@effect/ai API)
const prompt = Prompt.make([
  { role: "system", content: systemPrompt },
  { role: "user", content: userPrompt }
])

const model = yield* LanguageModel.LanguageModel
yield* model.generateObject(prompt, schema, config)
```

---

## Mock Layer Pattern (CRITICAL)

### Creating Mock LanguageModel for Tests

```typescript
import * as LanguageModel from "@effect/ai/LanguageModel"
import * as Layer from "effect/Layer"
import * as Effect from "effect/Effect"
import * as Stream from "effect/Stream"

const MockLanguageModelLayer = Layer.succeed(
  LanguageModel.LanguageModel,
  LanguageModel.LanguageModel.of({
    generate: (request) =>
      Stream.succeed({
        type: "success",
        content: "Mock response"
      }),

    generateObject: <A>(prompt, schema, options) =>
      Effect.succeed({
        // Return mock data matching schema
      } as A)
  })
)
```

### Complete Test Example

```typescript
import { effect, strictEqual } from "@beep/testkit"

const EntitySchema = S.Struct({
  name: S.String,
  type: S.Literal("person", "organization"),
  confidence: S.Number
})

// Mock Layer that returns predefined entities
const MockLlmLayer = Layer.succeed(
  LanguageModel.LanguageModel,
  LanguageModel.LanguageModel.of({
    generate: () => Stream.empty,
    generateObject: (prompt, schema, options) =>
      Effect.succeed([
        { name: "John", type: "person", confidence: 0.95 },
        { name: "Google", type: "organization", confidence: 0.92 }
      ])
  })
)

effect("extracts entities from text", () =>
  Effect.gen(function*() {
    const model = yield* LanguageModel.LanguageModel

    const entities = yield* model.generateObject(
      "John works at Google",
      S.Array(EntitySchema)
    )

    strictEqual(entities.length, 2)
    strictEqual(entities[0].name, "John")
  }).pipe(Effect.provide(MockLlmLayer))
)
```

---

## Provider Integration

### Anthropic (@effect/ai-anthropic)

```typescript
import { Anthropic } from "@effect/ai-anthropic"
import { LanguageModel } from "@effect/ai"

const AnthropicLayer = Anthropic.layer({
  apiKey: Config.string("ANTHROPIC_API_KEY")
})

Effect.runPromise(program.pipe(Effect.provide(AnthropicLayer)))
```

### OpenAI (@effect/ai-openai)

```typescript
import { OpenAI } from "@effect/ai-openai"
import { LanguageModel } from "@effect/ai"

const OpenAILayer = OpenAI.layer({
  apiKey: Config.string("OPENAI_API_KEY")
})

Effect.runPromise(program.pipe(Effect.provide(OpenAILayer)))
```

### Provider Selection Pattern

```typescript
const ProviderLayer = Config.string("AI_PROVIDER").pipe(
  Effect.map(provider => {
    switch (provider) {
      case "anthropic": return AnthropicLayer
      case "openai": return OpenAILayer
      default: throw new Error(`Unknown provider: ${provider}`)
    }
  }),
  Layer.unwrapEffect
)
```

---

## Advanced Features

### Prompt Caching (Anthropic-specific)

```typescript
const prompt = Prompt.make([
  {
    role: "system",
    content: "Long system prompt...",
    cache_control: { type: "ephemeral" }
  },
  {
    role: "user",
    content: "User query"
  }
])
```

### Streaming Responses

```typescript
const streamResponse = Effect.gen(function*() {
  const model = yield* LanguageModel.LanguageModel
  const stream = model.generate(request)

  yield* Stream.runForEach(stream, chunk => {
    console.log(chunk.content)
    return Effect.void
  })
})
```

### Token Usage Tracking

Token usage is returned in response objects for tracking costs and optimization.

---

## Key Takeaways

1. ✅ **System prompts ARE supported** via `Prompt.make()` with role-based messages
2. ✅ **No separate `generateObjectWithSystem` API** - use `Prompt.make()` for all prompts
3. ✅ **Mock layers use `LanguageModel.LanguageModel.of()`** for test doubles
4. ✅ **Provider-agnostic design** - swap Anthropic/OpenAI via Layer composition
5. ✅ **Schema validation built-in** - all responses validated against Effect Schema
6. ✅ **Effect-first design** - full integration with Layer/Context/Service patterns
